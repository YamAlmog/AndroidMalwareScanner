{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from statistics import mode\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1- Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data to pd \n",
    "df_path = \"android_apps_df.csv\"\n",
    "android_df = pd.read_csv(df_path)\n",
    "print(android_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = android_df.columns.tolist()\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set = set(features)\n",
    "print(len(features_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that there are no duplicate features in my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all of my features value are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are no missing values in my data at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove the sha256 names of the files because it is not informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df_copy = android_df.copy()\n",
    "# remove the column of files hash name beacause it is not informative for me\n",
    "android_df_copy = android_df_copy.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the most common features in apk files:<br>\n",
    "I'm using mean function to calculate the mean of each feature, then show the top ten of them. <br>\n",
    "It gives me information about the features that had the most one's, i.e the most common features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_features(df):\n",
    "    feature_means = df.mean()\n",
    "    sorted_features = feature_means.sort_values(ascending=False)\n",
    "    top_ten_features = sorted_features.head(10)\n",
    "    # Reverse the order for descending plot\n",
    "    top_ten_features = top_ten_features[::-1]\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    top_ten_features.plot(kind='barh', color='skyblue')\n",
    "    plt.xlabel('Mean value of features')\n",
    "    plt.title('10 Most common features')\n",
    "    plt.show()\n",
    "\n",
    "df_without_label = android_df_copy.drop('label', axis=1)\n",
    "most_common_features(df_without_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the distribution of positive labels amount in each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums_positive_label_of_each_features(df, label_column):   \n",
    "    # Filter rows where the label column is 1\n",
    "    positive_label_rows = df[df[label_column] == 1]\n",
    "    # Calculate the sum of each feature in the filtered rows\n",
    "    feature_sums = positive_label_rows.sum()\n",
    "    sorted_features = feature_sums.sort_values(ascending=False)\n",
    "    # Reverse the order for descending plot\n",
    "    sorted_features = sorted_features[::-1]\n",
    "    fig = px.bar(sorted_features, x=sorted_features.values, y=sorted_features.index, orientation='h',\n",
    "             title='Amount of positive labels for each feature',\n",
    "             labels={'x': 'Sum of Occurrences', 'y': 'Features'}, height=600, width=800)\n",
    "    fig.show()\n",
    "\n",
    "sums_positive_label_of_each_features(android_df_copy, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_dist(df, feature):    \n",
    "    plt.figure(figsize=(3,3))\n",
    "    value_counts = df[feature].value_counts()\n",
    "    plt.bar(value_counts.index, value_counts.values, color=['skyblue', 'orchid'])\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the label feature distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_dist(android_df_copy, 'label')\n",
    "android_df_copy[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display features distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_dist(df, feature_name):   \n",
    "    \n",
    "    fig=px.histogram(df, \n",
    "                    x=feature_name,\n",
    "                    color=\"label\",\n",
    "                    hover_data=df.columns,\n",
    "                    title=f\"Distribution of {feature_name} with label\",\n",
    "                    barmode=\"group\", \n",
    "                    width=600,\n",
    "                    height=400)\n",
    "    fig.show()\n",
    "\n",
    "features = android_df_copy.columns.tolist()\n",
    "\n",
    "for feature in features:\n",
    "    display_feature_dist(android_df_copy, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = android_df_copy['label']\n",
    "X = android_df_copy.drop('label', axis=1)\n",
    "X_train, X_test ,y_train, y_test= train_test_split(X, y, test_size = 0.20, random_state = 42, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'algorithm':['auto', 'ball_tree', 'kd_tree']}\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "knn_classifier = grid_search.best_estimator_\n",
    "\n",
    "'''# results: {'n_neighbors': 7, 'weights': 'distance'}\n",
    "knn_classifier=KNeighborsClassifier(n_neighbors=7, weights='distance')'''\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''rf_grid_params = {'criterion':['gini', 'entropy'], 'n_estimators': [60, 80, 100, 120], 'max_depth': [2,5,10], 'min_samples_split':[2,5,7]}\n",
    "rf_classifier = RandomForestClassifier()\n",
    "# Perform grid search\n",
    "rf_grid_search = GridSearchCV(rf_classifier, rf_grid_params, cv=5, scoring='accuracy')\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "# Get the best parameters\n",
    "best_params = rf_grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "rf_classifier = rf_grid_search.best_estimator_'''\n",
    "# results: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 120}\n",
    "rf_classifier = RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=2, n_estimators=120)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_params = {'C': [1,2,3], 'kernel': ['rbf', 'linear', 'poly']}\n",
    "svm_classifier = SVC()\n",
    "# Perform grid search\n",
    "svm_grid_search = GridSearchCV(svm_classifier, svm_grid_params, cv=5, scoring='accuracy')\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "# Get the best parameters\n",
    "best_params = svm_grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "svm_classifier = svm_grid_search.best_estimator_\n",
    "# result: {'C': 3, 'kernel': 'rbf'}\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with K fold cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_model_evaluation(X, y, models_list, k_fold=5):    \n",
    "    for model in models_list:\n",
    "        scores = cross_val_score(model, X, y, cv=k_fold, scoring='accuracy')\n",
    "        # Display the average performance score\n",
    "        print(f\"Average Accuracy of {model}:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models= [knn_classifier, rf_classifier, svm_classifier]\n",
    "cross_val_model_evaluation(X_train, y_train, models)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_one_model_evaluation(models_list, X_test, y_test):\n",
    "    for model in models_list:\n",
    "        y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f\"f1 score for {model} is {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_one_model_evaluation(models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_one_model_evaluation(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(models, X_train, y_train, X_test, y_test):\n",
    "    for model in models:\n",
    "        # Test set ROC\n",
    "        y_test_probability = model.predict_proba(X_test)\n",
    "        fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_probability[:, 1])\n",
    "\n",
    "        # Training set ROC\n",
    "        y_train_probability = model.predict_proba(X_train)\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_probability[:, 1])\n",
    "\n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr_train, tpr_train, color='green', label='Train ROC')\n",
    "        plt.plot(fpr_test, tpr_test, color='darkorange', label='Test ROC')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for {str(model)}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # AUC scores\n",
    "        auc_train = auc(fpr_train, tpr_train)\n",
    "        auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "        print(f'AUC (Train): {auc_train:.2f}')\n",
    "        print(f'AUC (Test): {auc_test:.2f}')\n",
    "\n",
    "model_list = [knn_classifier, rf_classifier]\n",
    "plot_roc_curve(model_list, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test set ROC\n",
    "y_test_probability = svm_classifier.predict(X_test)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_probability)\n",
    "\n",
    "# Training set ROC\n",
    "y_train_probability = svm_classifier.predict(X_train)\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_probability)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_train, tpr_train, color='green', label='Train ROC')\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange', label='Test ROC')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve for {str(svm_classifier)}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# AUC scores\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "print(f'AUC (Train): {auc_train:.2f}')\n",
    "print(f'AUC (Test): {auc_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(models, train_df, y_train, test_df, y_test):    \n",
    "    for model in models:    \n",
    "        model.fit(train_df, y_train)\n",
    "        predictions = model.predict(test_df)\n",
    "\n",
    "        conf_matrix = confusion_matrix(y_test, predictions)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actualy')\n",
    "        plt.title(f\"Confusion Matrix for {model}\")\n",
    "        plt.show()\n",
    "        \n",
    "models = [knn_classifier, rf_classifier, svm_classifier]\n",
    "show_confusion_matrix(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(rf_classifier, X_train, y_train, cv=5, train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy')\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, label=\"Training score\", marker='o')\n",
    "plt.plot(train_sizes, test_scores_mean, label=\"Test score\", marker='o')\n",
    "\n",
    "# Fill the area around the curves to represent the standard deviation\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"blue\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"orange\")\n",
    "\n",
    "plt.title(\"Learning Curves: Training and Test Error\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
