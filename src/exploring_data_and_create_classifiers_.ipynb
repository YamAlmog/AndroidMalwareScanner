{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1- Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data to pd \n",
    "df_path = \"C:/Projects/Python/Android\\AndroidMalwareScanner/dataframes/dataframe.csv\"\n",
    "test_df_path = \"C:/Projects/Python/Android/AndroidMalwareScanner/dataframes/test_set.csv\"\n",
    "android_df = pd.read_csv(df_path)\n",
    "test_df = pd.read_csv(test_df_path)\n",
    "print(android_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = android_df.columns.tolist()\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set = set(features)\n",
    "print(features_set)\n",
    "print(len(features_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that there are no duplicate features in my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all of my features value are numeric besides hash column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are no missing values in my dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove the sha256 names of the files because it is not informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df_copy = android_df.copy()\n",
    "# remove the column of files hash name beacause it is not informative for me\n",
    "android_df_copy = android_df_copy.drop('hash', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the most common features in apk files:<br>\n",
    "I'm using mean function to calculate the mean of each feature, then show the top ten of them. <br>\n",
    "It gives me information about the features that had the most one's, i.e the most common features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_features(df):\n",
    "    feature_means = df.mean()\n",
    "    sorted_features = feature_means.sort_values(ascending=False)\n",
    "    top_ten_features = sorted_features.head(10)\n",
    "    # Reverse the order for descending plot\n",
    "    top_ten_features = top_ten_features[::-1]\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    top_ten_features.plot(kind='barh', color='skyblue')\n",
    "    plt.xlabel('Mean value of features')\n",
    "    plt.title('10 Most common features')\n",
    "    plt.show()\n",
    "\n",
    "df_without_label = android_df_copy.drop('label', axis=1)\n",
    "most_common_features(df_without_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the distribution of positive labels amount in each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums_positive_label_of_each_features(df, label_column):   \n",
    "    # Filter rows where the label column is 1\n",
    "    positive_label_rows = df[df[label_column] == 1]\n",
    "    # Calculate the sum of each feature in the filtered rows\n",
    "    feature_sums = positive_label_rows.sum()\n",
    "    sorted_features = feature_sums.sort_values(ascending=False)\n",
    "    # Reverse the order for descending plot\n",
    "    sorted_features = sorted_features[::-1]\n",
    "    fig = px.bar(sorted_features, x=sorted_features.values, y=sorted_features.index, orientation='h',\n",
    "             title='Amount of positive labels for each feature',\n",
    "             labels={'x': 'Sum of Occurrences', 'y': 'Features'}, height=600, width=800)\n",
    "    fig.show()\n",
    "\n",
    "sums_positive_label_of_each_features(android_df_copy, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_dist(df, feature):    \n",
    "    value_counts = df[feature].value_counts().reset_index()\n",
    "    value_counts.columns = [feature, 'Count']\n",
    "\n",
    "    fig = px.bar(value_counts, x=feature, y='Count', color='Count',\n",
    "                labels={feature: feature, 'Count': 'Distribution'},\n",
    "                title=f'Distribution of {feature}')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the label feature distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_dist(android_df_copy, 'label')\n",
    "android_df_copy[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display features distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_dist(df, feature_name):   \n",
    "    \n",
    "    fig=px.histogram(df, \n",
    "                    x=feature_name,\n",
    "                    color=\"label\",\n",
    "                    hover_data=df.columns,\n",
    "                    title=f\"Distribution of {feature_name} with label\",\n",
    "                    barmode=\"group\", \n",
    "                    width=600,\n",
    "                    height=400)\n",
    "    fig.show()\n",
    "\n",
    "features = android_df_copy.columns.tolist()\n",
    "\n",
    "# for feature in features:\n",
    "#     display_feature_dist(android_df_copy, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the correlation matrix of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = android_df_copy.corr()\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', annot_kws={\"size\": 7}, linewidths=.5)\n",
    "plt.title('Correlation Matrix for All 50 Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some features that are in high correlation with each other for example: send sms and recieve sms or read sms with receive sms, it is clearly that i must use some feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = android_df_copy['label']\n",
    "X = android_df_copy.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display android_df label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().plot.pie(autopct='%.2f', colors=[ '#d62728', '#1f77b4'])\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle imbalanced dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# rus = RandomUnderSampler(sampling_strategy=0.5) # Numerical value\n",
    "# # rus = RandomUnderSampler(sampling_strategy=\"not minority\") # String\n",
    "# X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "# y_res.value_counts().plot.pie(autopct='%.2f',  colors=[ '#d62728', '#1f77b4'])\n",
    "# plt.title('Under-sampling Label Distribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#ros = RandomOverSampler(sampling_strategy=1) # Float\n",
    "ros = RandomOverSampler(sampling_strategy=\"not majority\") # String\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "y_res.value_counts().plot.pie(autopct='%.2f',  colors=[ '#d62728', '#1f77b4'])\n",
    "plt.title('Under-sampling Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split balanced data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test ,y_train, y_test= train_test_split(X_res, y_res, test_size = 0.30, random_state = 42, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape: {X_res.shape}, y shape: {y_res.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate test_df to label and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_df.copy()\n",
    "y_set = test_set['label']\n",
    "test_set = test_set.drop('label', axis=1)\n",
    "test_set = test_set.drop('hash', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_set shape: {test_set.shape}, y_set shape: {y_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display test_df label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_set.value_counts().plot.pie(autopct='%.2f', colors=[ '#d62728', '#1f77b4'])\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_set.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle imbalanced test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_res, y_test_res = ros.fit_resample(test_set, y_set)\n",
    "\n",
    "y_test_res.value_counts().plot.pie(autopct='%.2f',  colors=[ '#d62728', '#1f77b4'])\n",
    "plt.title('Under-sampling Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_res.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SelectKBest to implement feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "k_best = SelectKBest(score_func=f_classif, k=15)\n",
    "\n",
    "def fit_kbest_transform(classifier, X_train, y_train, X_test, k_best):\n",
    "    # Use SelectKBest with ANOVA F-statistic for feature selection\n",
    "    X_train_selected = k_best.fit_transform(X_train, y_train)\n",
    "    X_test_selected = k_best.transform(X_test)\n",
    "\n",
    "    # Fit the classifier on the selected features\n",
    "    classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "    return X_train_selected, X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectKBest is an Univariate feature selection, it works by selecting the best features based on univariate statistical tests. <br>\n",
    "SelectKBest removes all but the k highest scoring features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use PCA to implement feature selection / reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fit StandardScaler on the training data and transform it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same StandardScaler transformation to the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "X_test_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'algorithm':['auto', 'ball_tree', 'kd_tree']}\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "knn_classifier = grid_search.best_estimator_\n",
    "\n",
    "# knn_classifier.fit(X_train_pca, y_train)\n",
    "X_train_knn, X_test_knn = fit_kbest_transform(knn_classifier, X_train, y_train, X_test, k_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_knn[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see there are 10 features in x_train_knn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_params = {'criterion':['gini', 'entropy'], 'n_estimators': [80, 100, 110, 120], 'max_depth': [2,5,10], 'min_samples_split':[2,5,7]}\n",
    "rf_classifier = RandomForestClassifier()\n",
    "# Perform grid search\n",
    "rf_grid_search = GridSearchCV(rf_classifier, rf_grid_params, cv=5, scoring='accuracy')\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "# Get the best parameters\n",
    "best_params = rf_grid_search.best_params_\n",
    "print(best_params)\n",
    "rf_classifier = rf_grid_search.best_estimator_\n",
    "\n",
    "# rf_classifier.fit(X_train_pca, y_train)\n",
    "X_train_rf, X_test_rf = fit_kbest_transform(rf_classifier, X_train, y_train, X_test, k_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_params = {'C': [1, 2, 3], 'kernel': ['rbf', 'linear', 'poly']}\n",
    "svm_classifier = SVC()\n",
    "# Perform grid search\n",
    "svm_grid_search = GridSearchCV(svm_classifier, svm_grid_params, cv=5, scoring='accuracy')\n",
    "svm_grid_search.fit(X_train_pca, y_train)\n",
    "# Get the best parameters\n",
    "best_params = svm_grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "svm_classifier = svm_grid_search.best_estimator_\n",
    "\n",
    "svm_classifier.fit(X_train_pca, y_train)\n",
    "# X_train_svm, X_test_svm = fit_kbest_transform(svm_classifier, X_train, y_train, X_test, k_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"knn_train shape:{X_train_knn.shape}, Knn_test shape: {X_test_knn.shape}\")\n",
    "print(f\"rf_train shape:{X_train_rf.shape}, rf_test shape: {X_test_rf.shape}\")\n",
    "print(f\"svm_train shape:{X_train_pca.shape}, svm_test shape: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate with K fold cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_model_evaluation(X, y, models_list, k_fold=5):    \n",
    "    for model in models_list:\n",
    "        scores = cross_val_score(model, X, y, cv=k_fold, scoring='accuracy')\n",
    "        # Display the average performance score\n",
    "        print(f\"Average Accuracy of {model}:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models= [knn_classifier, rf_classifier, svm_classifier]\n",
    "cross_val_model_evaluation(X_res, y_res, models)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate with F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_one_model_evaluation(models_list, test_sets, y_test):\n",
    "    for model, X_test in zip(models_list, test_sets):\n",
    "        y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        print(f\"F1 score for {model.__class__.__name__} is {f1}\")\n",
    "\n",
    "# test_sets = [X_test_knn, X_test_rf, X_test_svm]\n",
    "test_sets = [X_test_knn, X_test_rf, X_test_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_one_model_evaluation(models, test_sets, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(models, train_set, test_sets, y_train, y_test):\n",
    "    for model, X_test, X_train, in zip(models, test_sets, train_set):\n",
    "        # Test set ROC\n",
    "        y_test_probability = model.predict_proba(X_test)\n",
    "        fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_probability[:, 1])\n",
    "\n",
    "        # Training set ROC\n",
    "        y_train_probability = model.predict_proba(X_train)\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_probability[:, 1])\n",
    "\n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr_train, tpr_train, color='green', label='Train ROC')\n",
    "        plt.plot(fpr_test, tpr_test, color='darkorange', label='Test ROC')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for {str(model)}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # AUC scores\n",
    "        auc_train = auc(fpr_train, tpr_train)\n",
    "        auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "        print(f'AUC (Train): {auc_train:.2f}')\n",
    "        print(f'AUC (Test): {auc_test:.2f}')\n",
    "\n",
    "model_list = [knn_classifier, rf_classifier]\n",
    "test_sets = [X_test_knn, X_test_rf]\n",
    "train_sets = [X_train_knn, X_train_rf]\n",
    "\n",
    "plot_roc_curve(model_list, train_sets, test_sets, y_train,  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_plot_roc_curve(X_train, y_train, X_test, y_test):\n",
    "    # Test set ROC\n",
    "    y_test_probability = svm_classifier.predict(X_test)\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_probability)\n",
    "\n",
    "    # Training set ROC\n",
    "    y_train_probability = svm_classifier.predict(X_train)\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_probability)\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr_train, tpr_train, color='green', label='Train ROC')\n",
    "    plt.plot(fpr_test, tpr_test, color='darkorange', label='Test ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {str(svm_classifier)}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # AUC scores\n",
    "    auc_train = auc(fpr_train, tpr_train)\n",
    "    auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    print(f'AUC (Train): {auc_train:.2f}')\n",
    "    print(f'AUC (Test): {auc_test:.2f}')\n",
    "\n",
    "\n",
    "# svm_plot_roc_curve(X_train_svm, y_train, X_test_svm, y_test)\n",
    "svm_plot_roc_curve(X_train_pca, y_train, X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(models, train_df, y_train, test_df, y_test):    \n",
    "    for model in models:    \n",
    "        model.fit(train_df, y_train)\n",
    "        predictions = model.predict(test_df)\n",
    "\n",
    "        conf_matrix = confusion_matrix(y_test, predictions)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actualy')\n",
    "        plt.title(f\"Confusion Matrix for {model}\")\n",
    "        plt.show()\n",
    "        \n",
    "models = [knn_classifier, rf_classifier, svm_classifier]\n",
    "show_confusion_matrix(models, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
